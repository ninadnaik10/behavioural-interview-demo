# SpeakSure - AI Automated Behavioral Interview System

An intelligent interview platform that leverages machine learning to analyze candidate responses in real-time, helping hiring managers make data-driven decisions during the recruitment process.

ğŸ¥ Demo Video: [https://drive.google.com/file/d/18E1ror1CspPB6zm4FAsQG4tsEK4KJR_5/view?usp=drive_link](https://drive.google.com/file/d/18E1ror1CspPB6zm4FAsQG4tsEK4KJR_5/view?usp=drive_link)

## ğŸ“‹ About The Project

SpeakSure is an AI-powered behavioral interview system designed to assist hiring managers in shortlisting candidates more efficiently. The system uses a custom-trained Multi-Layer Perceptron (MLP) model to analyze human speech patterns, confidence levels, and various linguistic parameters in real-time.

### Key Features

- **ğŸ¤ Real-time Audio Recording & Analysis**: Capture and analyze candidate responses during interviews
- **ğŸ§  AI-Powered Confidence Detection**: Custom MLP model trained on behavioral interview data to predict speaker confidence (1-5 scale)
- **ğŸ“ Automatic Transcription**: Leverages AssemblyAI for accurate speech-to-text conversion
- **âœï¸ Grammar Analysis**: Identifies and flags grammatical errors in responses
- **ğŸ“Š Speech Metrics**: Calculates words per minute, duration, and other speech characteristics
- **ğŸ¤– LLM-Powered Insights**: Uses Google Gemini to generate actionable feedback for hiring managers
- **ğŸ’¾ Persistent Storage**: MongoDB for storing interview results
- **ğŸ“ˆ Visual Analytics**: Interactive charts and dashboards for interview performance

## ğŸ—ï¸ Project Structure

```
speaksure-interview/
â”œâ”€â”€ client/                          # React Frontend Application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ Pages/                   # Main application pages
â”‚   â”‚   â”‚   â”œâ”€â”€ HomePage.tsx         # Landing page
â”‚   â”‚   â”‚   â”œâ”€â”€ InterviewPage.tsx    # Interview recording interface
â”‚   â”‚   â”‚   â””â”€â”€ InterviewResultPage.tsx  # Results visualization
â”‚   â”‚   â”œâ”€â”€ components/              # Reusable UI components (shadcn/ui)
â”‚   â”‚   â”‚   â”œâ”€â”€ ui/                  # Base UI components
â”‚   â”‚   â”‚   â””â”€â”€ theme-provider.tsx   # Theme management
â”‚   â”‚   â”œâ”€â”€ layout/                  # Layout components
â”‚   â”‚   â”œâ”€â”€ styles/                  # CSS modules
â”‚   â”‚   â””â”€â”€ lib/                     # Utility functions
â”‚   â”œâ”€â”€ package.json                 # Frontend dependencies
â”‚   â””â”€â”€ vite.config.ts               # Vite configuration
â”‚
â”œâ”€â”€ server/                          # Flask Backend Application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/                     # API layer
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.py            # API endpoints
â”‚   â”‚   â”‚   â””â”€â”€ services.py          # Business logic services
â”‚   â”‚   â”œâ”€â”€ core/                    # Core configuration
â”‚   â”‚   â”‚   â””â”€â”€ config.py            # App configuration
â”‚   â”‚   â””â”€â”€ models/                  # ML models
â”‚   â”‚       â”œâ”€â”€ trained_model.h5     # TensorFlow MLP model
â”‚   â”‚       â””â”€â”€ scaler.pkl           # Feature scaler
â”‚   â”œâ”€â”€ requirements.txt             # Python dependencies
â”‚   â””â”€â”€ run.py                       # Application entry point
â”‚
â””â”€â”€ venv/                            # Python virtual environment
```

## ğŸ› ï¸ Technology Stack

### Frontend

- **React 19** - UI library
- **TypeScript** - Type safety
- **Vite** - Build tool and dev server
- **TailwindCSS** - Styling
- **shadcn/ui** - UI component library
- **React Router DOM** - Navigation
- **Axios** - HTTP client
- **WaveSurfer.js** - Audio waveform visualization

### Backend

- **Flask** - Python web framework
- **TensorFlow** - Deep learning framework for MLP model
- **PyTorch** - Deep learning framework for Wav2Vec2
- **Transformers (Hugging Face)** - Wav2Vec2 audio feature extraction
- **librosa** - Audio processing and analysis
- **AssemblyAI** - Speech-to-text transcription
- **language-tool-python** - Grammar checking
- **MongoDB** - Database
- **Google Gemini** - LLM for generating insights
- **scikit-learn** - Feature scaling

### Machine Learning Pipeline

1. **Audio Feature Extraction**: Wav2Vec2 transformer model (`facebook/wav2vec2-base-960h`)
2. **Confidence Prediction**: Custom-trained MLP model
3. **Transcription**: AssemblyAI API
4. **Analysis**: Gemini 2.5 Flash for contextual feedback

## ğŸš€ Getting Started

### Prerequisites

- **Node.js** (v18 or higher)
- **Python** (v3.10 or higher)
- **MongoDB** (local or cloud instance)
- **API Keys**:
  - AssemblyAI API Key
  - Google Gemini API Key
  - MongoDB Connection URI

### Installation

#### 1. Clone the Repository

```bash
git clone <repository-url>
cd speaksure-interview
```

#### 2. Backend Setup

```bash
# Navigate to server directory
cd server

# Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate  # On macOS/Linux
# OR
venv\Scripts\activate  # On Windows

# Install dependencies
pip install -r requirements.txt

# Create .env file in the server directory
touch .env
```

Add the following environment variables to `.env`:

```env
FLASK_ENV=development
SECRET_KEY=your-secret-key
MONGO_URI=your-mongodb-connection-string
ASSEMBLYAI_API_KEY=your-assemblyai-api-key
GOOGLE_API_KEY=your-google-gemini-api-key
```

#### 3. Frontend Setup

```bash
# Open a new terminal and navigate to client directory
cd client

# Install dependencies
npm install
# OR
yarn install
```

### Running the Application

#### Start the Backend Server

```bash
# From the server directory with virtual environment activated
cd server
source venv/bin/activate  # Activate venv if not already active
python run.py
```

The Flask server will start on `http://localhost:5000`

#### Start the Frontend Development Server

```bash
# From the client directory (in a new terminal)
cd client
npm run dev
# OR
yarn dev
```

The React app will start on `http://localhost:5173` (or another port if 5173 is busy)

## ğŸ”§ Configuration

### Backend Configuration (`server/app/core/config.py`)

- `SAMPLE_RATE`: Audio sampling rate (default: 16000 Hz)
- `TRANSFORMER_MODEL_NAME`: Wav2Vec2 model identifier
- `MODEL_PATH`: Path to trained MLP model
- `SCALER_PATH`: Path to feature scaler
- `DATABASE_NAME`: MongoDB database name

### Frontend Configuration

Update the API base URL in your axios configuration if needed (typically in `client/src/` files).

## ğŸ“Š How It Works

1. **Interview Setup**: User enters candidate details and selects interview questions
2. **Audio Recording**: System records candidate's audio responses
3. **Real-time Processing**:
   - Audio is split into 10-second chunks
   - Wav2Vec2 extracts acoustic features
   - MLP model predicts confidence score for each chunk (1-5 scale)
4. **Transcription**: AssemblyAI converts speech to text
5. **Analysis**:
   - Grammar checker identifies linguistic errors
   - Speech rate calculated (words per minute)
   - Average confidence computed across all chunks
6. **AI Feedback**: Google Gemini generates critical feedback based on:
   - Answer relevancy to the question
   - Confidence levels
   - Speech rate
   - Overall coherence
7. **Results Storage**: All data saved to MongoDB for later review
8. **Visualization**: Interactive dashboard displays performance metrics

## ğŸ“ API Endpoints

- `POST /api/upload` - Upload and analyze audio recording
- `GET /api/results` - Retrieve all interview results
- `GET /api/results/<interview_id>` - Get specific interview results
- Additional endpoints in `server/app/api/routes.py`
